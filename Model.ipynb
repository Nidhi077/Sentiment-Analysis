{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSER USING NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer     \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading file and Processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('REVIEWS.txt',sep='\\t',names=['liked','txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liked</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   liked                                                txt\n",
       "0      1            The Da Vinci Code book is just awesome.\n",
       "1      1  this was the first clive cussler i've ever rea...\n",
       "2      1                   i liked the Da Vinci Code a lot.\n",
       "3      1                   i liked the Da Vinci Code a lot.\n",
       "4      1  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'she', 'weren', 'it', 'having', 'did', 'd', 'her', 'is', 'whom', 'should', 'y', 'ain', 'me', 'own', 'themselves', 'these', 'my', \"isn't\", 'between', \"needn't\", 'at', \"shan't\", 'hadn', 'what', 'about', 'theirs', 'again', 'each', 'wouldn', 'wasn', 'mustn', 'over', \"wouldn't\", 'any', 'same', 'where', \"you're\", 'isn', 'have', 'that', 'as', 'herself', 'when', 'does', 'once', 'in', 'while', 'few', 'ma', \"you've\", \"that'll\", 'here', 'only', 'before', 'doesn', \"doesn't\", 'most', 'i', 'his', 'he', 'been', 'through', \"haven't\", 'shouldn', 't', 'won', 'which', 'because', 'hasn', 'those', 'yourself', 'off', 'further', 'ourselves', 'who', 'be', 'they', 'or', 'all', \"weren't\", 'for', 'then', 'needn', 'after', \"don't\", 'them', 'how', 'your', 'can', 'm', 'but', 'to', 'down', 'don', \"hasn't\", \"shouldn't\", 'mightn', 'by', 'yours', 'you', 'why', 'some', 'our', 'other', 's', 'didn', 'itself', 'into', \"didn't\", \"she's\", 'are', 'do', \"should've\", 'than', 'myself', 'no', 're', \"wasn't\", 'now', 'out', \"it's\", 'am', 'has', \"aren't\", 'aren', 'above', 'if', 'their', 'an', 'on', 'until', 'such', 'will', 'hers', 'so', 'and', 'a', 'nor', 'below', 'both', \"you'll\", 'the', 'not', 'from', 'him', \"you'd\", 'this', 'with', 'shan', 'up', \"won't\", 'being', 'himself', \"couldn't\", 'yourselves', 'we', 'o', 'under', \"mustn't\", 'its', 'during', 'had', 'll', \"hadn't\", 'just', 'haven', 'against', 'of', \"mightn't\", 'ours', 'there', 'was', 'were', 'couldn', 'very', 'more', 've', 'too'}\n"
     ]
    }
   ],
   "source": [
    "stopset = set(stopwords.words(\"english\"))\n",
    "print (stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True,strip_accents='ascii',stop_words=stopset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this case, our dependent variable will be liked as 0(didn't like the movie) or 1(liked the movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "5       1\n",
      "6       1\n",
      "7       1\n",
      "8       1\n",
      "9       1\n",
      "10      1\n",
      "11      1\n",
      "12      1\n",
      "13      1\n",
      "14      1\n",
      "15      1\n",
      "16      1\n",
      "17      1\n",
      "18      1\n",
      "19      1\n",
      "20      1\n",
      "21      1\n",
      "22      1\n",
      "23      1\n",
      "24      1\n",
      "25      1\n",
      "26      1\n",
      "27      1\n",
      "28      1\n",
      "29      1\n",
      "       ..\n",
      "6888    0\n",
      "6889    0\n",
      "6890    0\n",
      "6891    0\n",
      "6892    0\n",
      "6893    0\n",
      "6894    0\n",
      "6895    0\n",
      "6896    0\n",
      "6897    0\n",
      "6898    0\n",
      "6899    0\n",
      "6900    0\n",
      "6901    0\n",
      "6902    0\n",
      "6903    0\n",
      "6904    0\n",
      "6905    0\n",
      "6906    0\n",
      "6907    0\n",
      "6908    0\n",
      "6909    0\n",
      "6910    0\n",
      "6911    0\n",
      "6912    0\n",
      "6913    0\n",
      "6914    0\n",
      "6915    0\n",
      "6916    0\n",
      "6917    0\n",
      "Name: liked, Length: 6918, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df.liked\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 418)\t0.3263290976383834\n",
      "  (0, 1906)\t0.3263290976383834\n",
      "  (0, 331)\t0.3262559657747791\n",
      "  (0, 216)\t0.7161106376265326\n",
      "  (0, 137)\t0.4095866691477198\n",
      "  (1, 418)\t0.09796474160548119\n",
      "  (1, 1906)\t0.09796474160548119\n",
      "  (1, 331)\t0.09794278725273431\n",
      "  (1, 674)\t0.22592190468842513\n",
      "  (1, 321)\t0.398401288038176\n",
      "  (1, 414)\t0.398401288038176\n",
      "  (1, 584)\t0.2298241967707875\n",
      "  (1, 1446)\t0.2238961227930013\n",
      "  (1, 582)\t0.28506337161183415\n",
      "  (1, 217)\t0.28195158685585064\n",
      "  (1, 1075)\t0.14687245685027675\n",
      "  (1, 1467)\t0.398401288038176\n",
      "  (1, 1360)\t0.398401288038176\n",
      "  (2, 418)\t0.23703245675471377\n",
      "  (2, 1906)\t0.23703245675471377\n",
      "  (2, 331)\t0.23697933668230028\n",
      "  (2, 1076)\t0.5507259552895343\n",
      "  (2, 1109)\t0.7267550791330575\n",
      "  (3, 418)\t0.23703245675471377\n",
      "  (3, 1906)\t0.23703245675471377\n",
      "  :\t:\n",
      "  (6913, 220)\t0.855108915546091\n",
      "  (6914, 1454)\t0.5724264186623724\n",
      "  (6914, 238)\t0.3318987022087672\n",
      "  (6914, 1211)\t0.3318987022087672\n",
      "  (6914, 463)\t0.6723202348298334\n",
      "  (6915, 1214)\t0.4220713593028687\n",
      "  (6915, 1222)\t0.2386399182125015\n",
      "  (6915, 1928)\t0.34153273272737517\n",
      "  (6915, 238)\t0.14552164048949137\n",
      "  (6915, 1211)\t0.14552164048949137\n",
      "  (6915, 135)\t0.3474840872714943\n",
      "  (6915, 1629)\t0.34826852533609814\n",
      "  (6915, 1220)\t0.34826852533609814\n",
      "  (6915, 1470)\t0.34826852533609814\n",
      "  (6915, 470)\t0.34826852533609814\n",
      "  (6916, 1214)\t0.40812482791804827\n",
      "  (6916, 1274)\t0.5870619498571349\n",
      "  (6916, 238)\t0.28142631891072306\n",
      "  (6916, 1211)\t0.28142631891072306\n",
      "  (6916, 887)\t0.5747963516894676\n",
      "  (6917, 1214)\t0.40820473350107567\n",
      "  (6917, 1273)\t0.5827067254117629\n",
      "  (6917, 238)\t0.28148141855807\n",
      "  (6917, 1211)\t0.28148141855807\n",
      "  (6917, 1770)\t0.5791011912613283\n"
     ]
    }
   ],
   "source": [
    "# convert df.txt from text to features\n",
    "x = vectorizer.fit_transform(df.txt)\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6918,)\n",
      "(6918, 2011)\n"
     ]
    }
   ],
   "source": [
    "#6918 observations x 2011 unique words.\n",
    "print (y.shape)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Train Split as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979292333245913"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, clf.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Our Model for any random review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_array1 = np.array([\"Jupiter Ascending was a disappointing and terrible movie\"])\n",
    "movie_review_vector = vectorizer.transform(movie_reviews_array1)\n",
    "print (clf.predict(movie_review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_array2 = np.array([\"saand ki ankh was an interesting story\"])\n",
    "movie_review_vector = vectorizer.transform(movie_reviews_array2)\n",
    "print (clf.predict(movie_review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_array3 = np.array([\"It was an amazing stuff\"])\n",
    "movie_review_vector = vectorizer.transform(movie_reviews_array3)\n",
    "print (clf.predict(movie_review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_array5 = np.array([\"That was a horrible one\"])\n",
    "movie_review_vector = vectorizer.transform(movie_reviews_array5)\n",
    "print (clf.predict(movie_review_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
